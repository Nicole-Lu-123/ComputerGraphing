{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4\n",
    "we first need to establish a vocabulary of visual words. We will form this vocabulary by sampling many local features from our training set and then clustering them with kmeans. \n",
    "1. You WILL want to use KMeans functions available in sci-kit learn library. The number of kmeans clusters is the size of our vocabulary and the size of our features. For example, you might start by clustering many SIFT descriptors into k=50 clusters. This partitions the continuous, 128 dimensional SIFT feature space into 50 regions. For any new SIFT feature we observe, we can figure out which region it belongs to as long as we save the centroids of our original clusters. \n",
    "2. We simply count how many SIFT descriptors fall into each cluster in our visual word vocabulary. This is done by finding the nearest neighbor kmeans centroid for every SIFT feature. Thus, if we have a vocabulary of 50 visual words, and we detect 220 SIFT features in an image, our bag of SIFT representation will be a histogram of 50 dimensions where each bin counts how many times a SIFT descriptor was assigned to that cluster and sums to 220. The histogram should be normalized so that image size does not dramatically change the bag of feature magnitude. To do this, you will need to fill in the details of get_bags_of_sifts function in the util.py; be sure to follow suggestions in the starter code.\n",
    "\n",
    "3. Plot average historam for every scene category. Average histograms can be obtained by simply averaging histograms for each training image. You should end up visualizing 15 average histograms which you should also submit as part of the writeup. Write a few sentences to describe how different are the histograms from different classes. Which classes you may believe to be hardest to separate (i.e., which you would be expect to be most confused) looking at these histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    \"\"\" Sample SIFT descriptors, cluster them using k-means, and return the fitted k-means model.\n",
    "    NOTE: We don't necessarily need to use the entire training dataset. You can use the function\n",
    "    sample_images() to sample a subset of images, and pass them into this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths: an (n_image, 1) array of image paths.\n",
    "    vocab_size: the number of clusters desired.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    kmeans: the fitted k-means clustering model.\n",
    "    \"\"\"\n",
    "    n_image = len(image_paths)\n",
    "\n",
    "    # Since want to sample tens of thousands of SIFT descriptors from different images, we\n",
    "    # calculate the number of SIFT descriptors we need to sample from each image.\n",
    "    n_each = int(np.ceil(10000 / n_image))\n",
    "\n",
    "    # Initialize an array of features, which will store the sampled descriptors\n",
    "    # keypoints = np.zeros((n_image * n_each, 2))\n",
    "    descriptors = np.zeros((n_image * n_each, 128))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        # Load features from each image\n",
    "        features = np.loadtxt(path, delimiter=',',dtype=float)\n",
    "        sift_keypoints = features[:, :2]\n",
    "        sift_descriptors = features[:, 2:]\n",
    "\n",
    "        # TODO: Randomly sample n_each descriptors from sift_descriptor and store them into descriptors\n",
    "        size = min(n_each,sift_descriptors.shape[0])\n",
    "        random_index = np.random.choice(sift_descriptors.shape[0], size, replace = False)\n",
    "\n",
    "        \n",
    "        for j, index in enumerate(random_index):\n",
    "            descriptors[i*n_each+j]=sift_descriptors[index]\n",
    "        \n",
    "\n",
    "    # TODO: pefrom k-means clustering to cluster sampled sift descriptors into vocab_size regions.\n",
    "    # You can use KMeans from sci-kit learn.\n",
    "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    kmeans = KMeans(n_clusters=vocab_size).fit(descriptors)\n",
    "    return kmeans\n",
    "    \n",
    "def get_bags_of_sifts(image_paths, kmeans):\n",
    "    \"\"\" Represent each image as bags of SIFT features histogram.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths: an (n_image, 1) array of image paths.\n",
    "    kmeans: k-means clustering model with vocab_size centroids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image_feats: an (n_image, vocab_size) matrix, where each row is a histogram.\n",
    "    \"\"\"\n",
    "    n_image = len(image_paths)\n",
    "    vocab_size = kmeans.cluster_centers_.shape[0]\n",
    "\n",
    "    image_feats = np.zeros((n_image, vocab_size))\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        # Load features from each image\n",
    "        features = np.loadtxt(path, delimiter=',',dtype=float)\n",
    "\n",
    "        # TODO: Assign each feature to the closest cluster center\n",
    "        # Again, each feature consists of the (x, y) location and the 128-dimensional sift descriptor\n",
    "        # You can access the sift descriptors part by features[:, 2:]\n",
    "        \n",
    "        sift_descriptors = features[:, 2:]\n",
    "        for sd in sift_descriptors:\n",
    "            pos = kmeans.predict([sd])\n",
    "            image_feats[i][pos] += 1 \n",
    "        # TODO: Build a histogram normalized by the number of descriptors\n",
    "\n",
    "        #normalize\n",
    "        image_feats[i] = image_feats[i]/np.sum(image_feats[i,:])\n",
    "        \n",
    "    return image_feats\n",
    "\n",
    "\n",
    "def avg_histogram(imgfeats,lables,category):\n",
    "    \"\"\"\n",
    "        plot the histogram base on the image_feats and array of class lables\n",
    "    \"\"\"\n",
    "    print(imgfeats)\n",
    "    avg_y = np.zeros((15,imgfeats.shape[1]))\n",
    "    print(avg_y.shape)\n",
    "    bag, count = np.unique(lables, return_counts = True)\n",
    "    for i, lable in enumerate(lables):\n",
    "        index = np.where(bag == lable)\n",
    "        avg_y[index] += imgfeats[i]\n",
    "    \n",
    "    # avg: divide the whole count\n",
    "    for j in range(15):\n",
    "        avg_y[j] = avg_y[j]/count[j]\n",
    "        \n",
    "    for k in range(15):\n",
    "        plt.hist(np.arange(imgfeats[1]), avg_y[0], facecolor='orange')\n",
    "        plt.title(\"Average Histogram for:\" + category[i])\n",
    "        plt.savefig('avghistogramFor/' + category[i] + '.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_category(ds_path):\n",
    "    classes = glob.glob(os.path.join(ds_path, \"*\"))\n",
    "    category = []\n",
    "    for c in classes:\n",
    "#         print(c.split(\"\\\\\")[1])\n",
    "        ctg = str(c.split(\"\\\\\")[1])\n",
    "        category.append(ctg)\n",
    "    return category\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred,normalize,title):\n",
    "    # ref from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix#sklearn.metrics.confusion_matrix\n",
    "     \n",
    "    cm = confusion_matrix(y_true,y_pred,title=title)\n",
    "    print(cm)\n",
    "    \n",
    "    accuracy = sum(cm.diagonal())/cm.sum()\n",
    "    \n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    \n",
    "    plt.title(title+accuracy)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"confusionmatrix optput\"+ title+accuracy+\".png\")\n",
    "\n",
    "def load(ds_path):\n",
    "    \"\"\" Load from the training/testing dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_path: path to the training/testing dataset.\n",
    "             e.g., sift/train or sift/test \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_paths: a (n_sample, 1) array that contains the paths to the descriptors. \n",
    "    labels: class labels corresponding to each image\n",
    "    \"\"\"\n",
    "    # Grab a list of paths that matches the pathname\n",
    "    files = glob.glob(os.path.join(ds_path, \"*\", \"*.txt\"))\n",
    "    n_files = len(files)\n",
    "    image_paths = np.asarray(files)\n",
    " \n",
    "    # Get class labels\n",
    "    classes = glob.glob(os.path.join(ds_path, \"*\"))\n",
    "    labels = np.zeros(n_files)\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        folder, fn = os.path.split(path)\n",
    "        labels[i] = np.argwhere(np.core.defchararray.equal(classes, folder))[0,0]\n",
    "\n",
    "    # Randomize the order\n",
    "    idx = np.random.choice(n_files, size=n_files, replace=False)\n",
    "    image_paths = image_paths[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths, labels = load(\"sift/train\")\n",
    "    #build_vocabulary(paths, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get_category(\"sift/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5\n",
    "You should now measure how well your bag of SIFT representation works when paired with a nearest neighbor classifier. There are many design decisions and free parameters for the bag of SIFT representation so accuracy might vary from 30% to 40%. To implement this part follow the instructions in classifiers.py. Again, you will want to use sci-kit learn library instead of coding this from scratch yourself.\n",
    "\n",
    "To measure performance of nearest neighbor classifier report clssification accuracy and plot confusion matrix (which should be of size 15x15). You will be required to hand in both of these measures as part of your PDF writeup. Experiment with the size of k, how does this effect the performance of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Starter code prepared by Borna Ghotbi for computer vision\n",
    " #based on MATLAB code by James Hay\n",
    "\n",
    "'''This function will predict the category for every test image by finding\n",
    "the training image with most similar features. Instead of 1 nearest\n",
    "neighbor, you can vote based on k nearest neighbors which will increase\n",
    "performance (although you need to pick a reasonable value for k). '''\n",
    "\n",
    "def nearest_neighbor_classify(k,train_image_feats, train_labels, test_image_feats):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "        ----------\n",
    "        train_image_feats:  is an N x d matrix, where d is the dimensionality of the feature representation.\n",
    "        train_labels: is an N x l cell array, where each entry is a string \n",
    "        \t\t\t  indicating the ground truth one-hot vector for each training image.\n",
    "    \ttest_image_feats: is an M x d matrix, where d is the dimensionality of the\n",
    "    \t\t\t\t\t  feature representation. You can assume M = N unless you've modified the starter code.\n",
    "        \n",
    "    Returns\n",
    "        -------\n",
    "    \tis an M x l cell array, where each row is a one-hot vector \n",
    "        indicating the predicted category for each test image.\n",
    "\n",
    "    Usefull funtion:\n",
    "    \t\n",
    "    \t# You can use knn from sci-kit learn.\n",
    "        # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    '''\n",
    "    # make k is 4\n",
    "#     k = 4\n",
    "    knn = KNeighborsClassifier(n_neighbors=k).fit(train_image_feats,train_labels)\n",
    "    predicted_labels = knn.predict(test_image_feats)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "'''This function will train a linear SVM for every category (i.e. one vs all)\n",
    "and then use the learned linear classifiers to predict the category of\n",
    "very test image. Every test feature will be evaluated with all 15 SVMs\n",
    "and the most confident SVM will \"win\". Confidence, or distance from the\n",
    "margin, is W*X + B where '*' is the inner product or dot product and W and\n",
    "B are the learned hyperplane parameters. '''\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "        ----------\n",
    "        train_image_feats:  is an N x d matrix, where d is the dimensionality of the feature representation.\n",
    "        train_labels: is an N x l cell array, where each entry is a string \n",
    "        \t\t\t  indicating the ground truth one-hot vector for each training image.\n",
    "    \ttest_image_feats: is an M x d matrix, where d is the dimensionality of the\n",
    "    \t\t\t\t\t  feature representation. You can assume M = N unless you've modified the starter code.\n",
    "        \n",
    "    Returns\n",
    "        -------\n",
    "    \tis an M x l cell array, where each row is a one-hot vector \n",
    "        indicating the predicted category for each test image.\n",
    "\n",
    "    Usefull funtion:\n",
    "    \t\n",
    "    \t# You can use svm from sci-kit learn.\n",
    "        # Reference: https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "    '''\n",
    "    svmclassfier = OneVsRestClassifier(svm.SVC(kernel=\"linear\", C=150)).fit(train_image_feats,train_labels)\n",
    "    predicted_labels = svmclassfier.predict(test_image_feats)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6\n",
    "\n",
    "1. There are numerous methods to learn linear classifiers but we will find linear decision boundaries with a support vector machine. You do not have to implement the support vector machine. However, linear classifiers are inherently binary and we have a 15-way classification problem. To decide which of 15 categories a test case belongs to, you will train 15 binary, 1-vs-all SVMs. 1-vs-all means that each classifier will be trained to recognize 'forest' vs 'non-forest', 'kitchen' vs 'non-kitchen', etc. All 15 classifiers will be evaluated on each test case and the classifier which is most confidently positive \"wins\". E.g. if the 'kitchen' classifier returns a score of -0.2 (where 0 is on the decision boundary), and the 'forest' classifier returns a score of -0.3, and all of the other classifiers are even more negative, the test case would be classified as a kitchen even though none of the classifiers put the test case on the positive side of the decision boundary. When learning an SVM, you have a free parameter 'C' which controls how strongly regularized the model is. Your accuracy will be very sensitive to C, so be sure to test differnt values. See classifiers.py for more details.\n",
    "\n",
    "2. Now you can evaluate the bag of SIFT representation paired with 1-vs-all linear SVMs. Accuracy should be from 40% to 50% depending on the parameters. To measure performance of the SVM classifier report clssification accuracy and plot confusion matrix (which should be of size 15x15) again. You will be required to hand in both of these measures as part of your PDF writeup. Experiment with the parameter 'C' and report your experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting paths and labels for all train and test data\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-0154258f381e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# #TODO: You code get_bags_of_sifts function in util.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtrain_image_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bags_of_sifts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_image_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtest_image_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bags_of_sifts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#If you want to avoid recomputing the features while debugging the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-4cb499b3127d>\u001b[0m in \u001b[0;36mget_bags_of_sifts\u001b[1;34m(image_paths, kmeans)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0msift_descriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msift_descriptors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mimage_feats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# TODO: Build a histogram normalized by the number of descriptors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, sample_weight)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         \u001b[0mx_squared_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m         return _labels_inertia(X, sample_weight, x_squared_norms,\n\u001b[0m\u001b[0;32m   1205\u001b[0m                                self.cluster_centers_, self._n_threads)[0]\n\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_labels_inertia\u001b[1;34m(X, sample_weight, x_squared_norms, centers, n_threads)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0m_inertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_inertia_dense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m     _labels(X, sample_weight, x_squared_norms, centers, centers,\n\u001b[0m\u001b[0;32m    659\u001b[0m             \u001b[0mweight_in_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_shift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             update_centers=False)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Starter code prepared by Borna Ghotbi, Polina Zablotskaia, and Ariel Shann for Computer Vision\n",
    "#based on a MATLAB code by James Hays and Sam Birch \n",
    "\n",
    "import numpy as np\n",
    "# from util import load, build_vocabulary, get_bags_of_sifts\n",
    "from classifiers import nearest_neighbor_classify, svm_classify\n",
    "\n",
    "#For this assignment, you will need to report performance for sift features on two different classifiers:\n",
    "# 1) Bag of sift features and nearest neighbor classifier\n",
    "# 2) Bag of sift features and linear SVM classifier\n",
    "\n",
    "#For simplicity you can define a \"num_train_per_cat\" vairable, limiting the number of\n",
    "#examples per category. num_train_per_cat = 100 for intance.\n",
    "\n",
    "#Sample images from the training/testing dataset. \n",
    "#You can limit number of samples by using the n_sample parameter.\n",
    "\n",
    "print('Getting paths and labels for all train and test data\\n')\n",
    "train_image_paths, train_labels = load(\"sift/train\")\n",
    "test_image_paths, test_labels = load(\"sift/test\")\n",
    "# print(train_labels)\n",
    "# print(test_labels)\n",
    "\n",
    "''' Step 1: Represent each image with the appropriate feature\n",
    " Each function to construct features should return an N x d matrix, where\n",
    " N is the number of paths passed to the function and d is the \n",
    " dimensionality of each image representation. See the starter code for\n",
    " each function for more details. '''\n",
    "\n",
    "        \n",
    "# print('Extracting SIFT features\\n')\n",
    "# #TODO: You code build_vocabulary function in util.py\n",
    "kmeans = build_vocabulary(train_image_paths, vocab_size=200)\n",
    "# print(\"Done SIPFT\")\n",
    "# #TODO: You code get_bags_of_sifts function in util.py \n",
    "train_image_feats = get_bags_of_sifts(train_image_paths, kmeans)\n",
    "test_image_feats = get_bags_of_sifts(test_image_paths, kmeans)\n",
    "        \n",
    "#If you want to avoid recomputing the features while debugging the\n",
    "#classifiers, you can either 'save' and 'load' the extracted features\n",
    "#to/from a file.\n",
    "\n",
    "np.save(\"train_image_feats\", train_image_feats)\n",
    "np.save(\"test_image_feats\", test_image_feats)\n",
    "print(\"Done save\")\n",
    "train_image_feats = np.load(\"train_image_feats.npy\")\n",
    "test_image_feats = np.load(\"test_image_feats.npy\")\n",
    "print(\"show the histogram\")\n",
    "category = get_category(\"sift/train\")\n",
    "avg_histogram(test_image_feats,test_labels,category)\n",
    "print(\"Done histogram\")\n",
    "\n",
    "# ''' Step 2: Classify each test image by training and using the appropriate classifier\n",
    "#  Each function to classify test features will return an N x l cell array,\n",
    "#  where N is the number of test cases and each entry is a string indicating\n",
    "#  the predicted one-hot vector for each test image. See the starter code for each function\n",
    "#  for more details. '''\n",
    "\n",
    "print('Using nearest neighbor classifier to predict test set categories\\n')\n",
    "#TODO: YOU CODE nearest_neighbor_classify function from classifers.py\n",
    "pred_labels_knn = nearest_neighbor_classify(4,train_image_feats, train_labels, test_image_feats)\n",
    "  \n",
    "\n",
    "print('Using support vector machine to predict test set categories\\n')\n",
    "#TODO: YOU CODE svm_classify function from classifers.py\n",
    "pred_labels_svm = svm_classify(train_image_feats, train_labels, test_image_feats)\n",
    "\n",
    "\n",
    "\n",
    "print('---Evaluation---\\n')\n",
    "# Step 3: Build a confusion matrix and score the recognition system for \n",
    "#         each of the classifiers.\n",
    "# TODO: In this step you will be doing evaluation. \n",
    "# 1) Calculate the total accuracy of your model by counting number\n",
    "#   of true positives and true negatives over all. \n",
    "# 2) Build a Confusion matrix and visualize it. \n",
    "#   You will need to convert the one-hot format labels back\n",
    "#   to their category name format.\n",
    "knn_correct = np.sum(pred_labels_knn == test_labels)\n",
    "svm_correct = np.sum(pred_labels_svm == test_labels)\n",
    "\n",
    "print(\"KNN Accurracy:\", knn_correct / len(test_labels))\n",
    "print(\"SVM Accurracy:\", svm_correct / len(test_labels))\n",
    "\n",
    "knn_cm = plot_confusion_matrix(test_labels, pred_labels_knn, \"KNN Accurracy:\")\n",
    "svn_cm = plot_confusion_matrix(test_labels, pred_labels_svm, \"SVM Accurracy:\")\n",
    "\n",
    "# Interpreting your performance with 100 training examples per category:\n",
    "#  accuracy  =   0 -> Your code is broken (probably not the classifier's\n",
    "#                     fault! A classifier would have to be amazing to\n",
    "#                     perform this badly).\n",
    "#  accuracy ~= .10 -> Your performance is chance. Something is broken or\n",
    "#                     you ran the starter code unchanged.\n",
    "#  accuracy ~= .40 -> Rough performance with bag of SIFT and nearest\n",
    "#                     neighbor classifier. \n",
    "#  accuracy ~= .50 -> You've gotten things roughly correct with bag of\n",
    "#                     SIFT and a linear SVM classifier.\n",
    "#  accuracy >= .60 -> You've added in spatial information somehow or you've\n",
    "#                     added additional, complementary image features. This\n",
    "#                     represents state of the art in Lazebnik et al 2006.\n",
    "#  accuracy >= .85 -> You've done extremely well. This is the state of the\n",
    "#                     art in the 2010 SUN database paper from fusing many \n",
    "#                     features. Don't trust this number unless you actually\n",
    "#                     measure many random splits.\n",
    "#  accuracy >= .90 -> You used modern deep features trained on much larger\n",
    "#                     image databases.\n",
    "#  accuracy >= .96 -> You can beat a human at this task. This isn't a\n",
    "#                     realistic number. Some accuracy calculation is broken\n",
    "#                     or your classifier is cheating and seeing the test\n",
    "#                     labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran it and waited about 1 hour, no response, it was just frozen......I tried to run just one function, but still nothing responded.So I submit it after I code the needed function without the result graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
